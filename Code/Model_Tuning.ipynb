{"cells":[{"cell_type":"code","source":["%run \"./Forecasting_Models\""],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6b744bb-ca16-4050-86d9-e2d5444267c7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run \"./General_Functions\""],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"986b6307-1f05-4ab1-8879-2f69ffa8373e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run \"./Performance_Metrics\""],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aca4468-27a8-48b7-a982-d94f8a5ccc51"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Notebook with functions to perform hyperparameters tuning of forecasting models"],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5da4443-6fe1-427c-a952-d268882e61af"}}},{"cell_type":"markdown","source":["This notebook contains all the functions related to the hyperparameters tuning process of the solution, where in this\ncontext tuning involves the training, validation and selection steps. This process is done using the HyperOpt\noptimization framework, which uses a bayesian optimization approach to perform an informed search over the solution\nspace in order to find the best possible set of hyperparameters.\n\nAs the modelling stage is done at the product (SKU) level, the hyperparameter optimization process happens independently\nfor each product.\n\nThe functions included are:\n\n| Function | Description |\n| -------- | ----------- |\n| `objective_function` | objective function used within the hyperparameters optimization process. This function defines, trains and evaluates a forecasting model as part of the procedure that happens during tuning |\n| `tune_ts_model` | performs hyperparameter tuning of a forecasting model using the bayesian optimization framework \"HyperOpt\" |"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de142576-cfd6-4bbf-a122-3dd95a23cc2e"}}},{"cell_type":"markdown","source":["###### Definition of functions"],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fa65cbb-f64d-497d-8113-3f03e6a5552b"}}},{"cell_type":"code","source":["def objective_function(params, data, model_name, start_val, end_val, holidays=False, df_frds=None):\n    \"\"\"\n    Defines, trains and validates a forecasting model defined by the algorithm in \"model_name\" and the hyperparameters\n    given in \"params\". First the input series is split into train and validation windows, then the model is trained with\n    the train portion and validated with the validation portion using the WAPE metric.\n\n    This function serves as the objective function to optimize during tuning, where the optimized metric is the\n    validation WAPE.\n\n    Parameters\n    __________\n        params (dict): Dictionary with the hyperparameters to use to define the model.\n        data (pd.DataFrame): Dataset with the time series.\n        model_name (str): Algorithm to use to define the model.\n        start_val (str): Start of validation set (included).\n        end_val (str): End of validation set (included).\n        holidays (bool, defaults to False): Flag to indicate whether the holidays are included or not.\n        df_holis (pd.DataFrame, defaults to None): Dataset with holidays.\n\n    Returns\n    _______\n        dict : Dictionary with the results of the model defined by \"params\". This include:\n            - Validation WAPE (objective metric)\n            - Train WAPE\n    \"\"\"\n    # Splitting the series\n    df_train, df_val = split_series(data, start_val, end_val)\n\n    # Validating if holidays are included\n    if holidays and (model_name == \"sarimax\"):\n        exog = df_val[\"holiday\"].values\n    elif holidays and (model_name == \"prophet\"):\n        df_holidays = df_frds.copy()\n    else:\n        exog = None\n        df_holidays = None\n\n    # Validating algorithm to use\n    if model_name == \"sarimax\":\n        # Obtaining trained SARIMAX model\n        model = obtain_sarimax(df_train, params, holidays=holidays)\n\n        # Retrieving fitted values for train set\n        train_fcsts = model.fittedvalues\n\n        # Generating forecast for validation set\n        val_fcsts = model.predict(\n            start=len(df_train),\n            end=len(df_train) + len(df_val) - 1,\n            exog=exog,\n            typ=\"levels\"\n        )\n\n        # Calculating performance metrics\n        train_wape = wape(df_train[\"y\"].values, train_fcsts.values)\n        val_wape = wape(df_val[\"y\"].values, val_fcsts.values)\n\n    elif model_name == \"prophet\":\n        # Obtaining trained Prophet model\n        model = obtain_prophet(df_train, params, df_holidays=df_holidays)\n\n        # Generating forecast for the entire series\n        val_fcsts = model.predict(data)[[\"ds\", \"yhat\"]]\n\n        # Splitting into fitted values and forecasts\n        train_fcsts = val_fcsts[val_fcsts[\"ds\"] < start_val]\n        val_fcsts = val_fcsts[val_fcsts[\"ds\"] >= start_val]\n\n        # Calculating performance metrics\n        train_wape = wape(df_train[\"y\"].values, train_fcsts[\"yhat\"].values)\n        val_wape = wape(df_val[\"y\"].values, val_fcsts[\"yhat\"].values)\n\n    else:\n        raise RuntimeError(\"Specified algorithm is not supported or it wasnt entered correctly\")\n\n    return {\"loss\": val_wape, \"train_wape\": train_wape, \"params\": params, \"status\": STATUS_OK}"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7be06e5-bf13-43f6-8998-bb06ed2e4834"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def tune_ts_model(model_name, search_space, num_evals, data, start_val, end_val, holidays=False, df_frds=None):\n    \"\"\"\n    Performs hyperparameter tuning of a forecasting model based on native time series techniques using the bayesian\n    optimization framework \"HyperOpt\".\n\n    The metric to optimize during tuning is the WAPE on the validation set, which is done by means of a common single\n    validation set approach.\n\n    Parameters\n    __________\n        model_name (str): Algorithm to use to define the model.\n        search_space (dict): Dictionary with the hyperparameter search space defined in hyperopt.\n        num_evals (int): Number of evaluations to perform over the search space during tuning.\n        data (pd.DataFrame): Dataset with the time series.\n        start_val (str): Start of validation set (included).\n        end_val (str): End of validation set (included).\n        holidays (bool, defaults to False): Flag to indicate whether the holidays are included or not.\n        df_holis (pd.DataFrame, defaults to None): Dataset with holidays.\n\n    Returns\n    _______\n        dict : Dictionary with the best results found after the optimization. This includes:\n            - Validation WAPE\n            - Train WAPE\n            - Set of the best hyperparameters that produce the above metrics.\n    \"\"\"\n    # Defining optimization algorithm and results object\n    optimizer = tpe.suggest\n    trials = Trials()\n\n    # Defining objective function with additional arguments\n    mod_function = partial(\n        objective_function,\n        data=data,\n        model_name=model_name,\n        start_val=start_val,\n        end_val=end_val,\n        holidays=holidays,\n        df_frds=df_frds\n    )\n\n    # Performing optimization for model tuning\n    argmin = fmin(\n        fn=mod_function,\n        space=search_space,\n        algo=optimizer,\n        max_evals=num_evals,\n        trials=trials,\n        verbose=False\n    )\n\n    # Extracting optimization results\n    df_results = pd.DataFrame(trials.results)\n\n    # Sorting according to validation results\n    df_results = df_results.sort_values(by=\"loss\", ascending=True).reset_index(drop=True)\n\n    return {\"train_wape\": df_results[\"train_wape\"][0], \"val_wape\": df_results[\"loss\"][0],\"params\": df_results[\"params\"][0]}\n"],"metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f602742-3066-443a-baf9-17a6d4137e64"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.6","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"Model_Tuning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":154105517223486}},"nbformat":4,"nbformat_minor":0}
