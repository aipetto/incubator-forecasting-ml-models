{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, IntegerType\n",
    "from pyspark.sql.functions import year, month, dayofweek, avg, coalesce, expr, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "  \"\"\"\n",
    "  Function that does pre-processing of the data, filling for potential NULLs in the data\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  df: pyspark.sql.dataframe.DataFrame\n",
    "    Dataframe with raw information\n",
    "    \n",
    "  Returns\n",
    "  ----------\n",
    "  df: pyspark.sql.dataframe.DataFrame\n",
    "    Dataframe with cleaned information, and no NULLs\n",
    "  \"\"\"\n",
    "  # Change data types\n",
    "  df = df\\\n",
    "  .withColumn(\"date\", df['date'].cast(DateType()))\\\n",
    "  .withColumn(\"demand\", df['demand'].cast(IntegerType()))  \n",
    "  \n",
    "  # Order registers by SKU and then, date\n",
    "  df = df.orderBy(\n",
    "    \"n_sku\",\n",
    "    \"date\"\n",
    "  )\n",
    "  \n",
    "  # To fill NULLs, we'll calculate an average by year, month and weekday. \n",
    "  # To do this, we start by calculating year, month and weekday columns\n",
    "  df = df\\\n",
    "    .withColumn(\"year\", year(df['date']))\\\n",
    "    .withColumn(\"month\", month(df['date']))\\\n",
    "    .withColumn(\"weekday\", dayofweek(df['date']))\n",
    "  \n",
    "  # Group by and calculate average\n",
    "  df_avg = df.groupBy(\n",
    "    \"n_sku\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"weekday\"\n",
    "  ).agg(\n",
    "    avg(\"demand\").alias(\"demand_avg\")\n",
    "    )\n",
    "  \n",
    "  # Merge base dataframe with df_avg\n",
    "  df = df.join(df_avg, ['n_sku', 'month', 'year', 'weekday'])\n",
    "  \n",
    "  # Fill NULLs\n",
    "  df = df.withColumn(\"demand\", coalesce(\"demand\", \"demand_avg\"))\n",
    "  \n",
    "  # Keep only relevant columns\n",
    "  df = df.select(\n",
    "    \"date\",\n",
    "    \"n_sku\",\n",
    "    \"demand\"\n",
    "  )\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "  \"\"\"\n",
    "  Function that removes outliers, finding them statistically after de-trending and de-seasonalizing each time series\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  df: pyspark.sql.dataframe.DataFrame\n",
    "    Dataframe with outliers\n",
    "    \n",
    "  Returns\n",
    "  ----------\n",
    "  df: pyspark.sql.dataframe.DataFrame\n",
    "    Dataframe without outliers\n",
    "  \"\"\"\n",
    "  # First, we de-seasonalize the series, dividing each value by the weekday average\n",
    "  # To do this, we start by calculating the average per weekday\n",
    "  df = df\\\n",
    "    .withColumn(\"year\", year(df['date']))\\\n",
    "    .withColumn(\"month\", month(df['date']))\\\n",
    "    .withColumn(\"weekday\", dayofweek(df['date']))\n",
    "  \n",
    "  df_avg_weekday = df.groupBy(\n",
    "    \"n_sku\",\n",
    "    \"weekday\"\n",
    "  ).agg(\n",
    "    avg(\"demand\").alias(\"avg_per_weekday\")\n",
    "    )\n",
    "  \n",
    "  # We de-seasonalize, dividing by this average\n",
    "  df = df.join(df_avg_weekday, [\"n_sku\", \"weekday\"])\n",
    "  df = df\\\n",
    "    .withColumn(\"demand_de_seas\", df[\"demand\"]/df[\"avg_per_weekday\"])\n",
    "  \n",
    "  # We now repeat the same, calculating the average per year and month of the de-seasonalized series\n",
    "  df_avg_month = df.groupBy(\n",
    "    \"n_sku\",\n",
    "    \"year\",\n",
    "    \"month\"\n",
    "  ).agg(\n",
    "    avg(\"demand_de_seas\").alias(\"avg_per_month\")\n",
    "  )\n",
    "  df = df.join(df_avg_month, [\"n_sku\", \"year\", \"month\"])\n",
    "  df = df\\\n",
    "    .withColumn(\"demand_de_trend\", df[\"demand_de_seas\"]/df[\"avg_per_month\"])\n",
    "  \n",
    "  # Calculate percentiles by DFU\n",
    "  df_perc = df.groupBy(\n",
    "    \"n_sku\"\n",
    "  ).agg(\n",
    "    expr('percentile(demand_de_trend, array(0.75))')[0].alias(\"Q3\"),\n",
    "    expr('percentile(demand_de_trend, array(0.25))')[0].alias(\"Q1\")\n",
    "    )\n",
    "  # A point will be an outlier if\n",
    "  # - It is greater than ThirdQuartile + 3*InterQuartileRange\n",
    "  # - It is smaller than FirstQuartile - 3*InterQuartileRange\n",
    "  df_perc = df_perc\\\n",
    "    .withColumn(\"HighOutlierThreshold\", df_perc[\"Q3\"] + 3*(df_perc['Q3'] - df_perc[\"Q1\"]))\\\n",
    "    .withColumn(\"LowOutlierThreshold\", df_perc[\"Q1\"] - 3*(df_perc['Q3'] - df_perc[\"Q1\"]))\n",
    "  \n",
    "  # Join information\n",
    "  df = df.join(df_perc, [\"n_sku\"])\n",
    "  \n",
    "  # Create outlier indicator column\n",
    "  df = df.withColumn(\"outlier_flag\", \n",
    "                      (when(df['demand_de_trend'] > df['HighOutlierThreshold'], 1)\n",
    "                      .when(df['demand_de_trend'] < df['LowOutlierThreshold'], 1)\n",
    "                      .otherwise(0))\n",
    "                    )\n",
    "  # We will replace outliers with an average per weekday, year and month\n",
    "  df_avg = df.groupBy(\n",
    "    \"n_sku\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"weekday\"\n",
    "  ).agg(\n",
    "    avg(\"demand\").alias(\"avg_per_month_weekday\")\n",
    "  )\n",
    "  df = df.join(df_avg, [\"n_sku\", \"year\", \"month\", \"weekday\"])\n",
    "    \n",
    "  df = df.withColumn(\"demand_corrected\", \n",
    "                    (when(df['outlier_flag'] == 1, df['avg_per_month_weekday'])\n",
    "                    .otherwise(df['demand']))\n",
    "                  )\n",
    "  # Rename columns and select what we want\n",
    "  df = df.withColumn('demand', df['demand_corrected'])\n",
    "  df = df.select(\n",
    "  \"date\",\n",
    "  \"n_sku\",\n",
    "  \"demand\"\n",
    ")\n",
    "\n",
    "  return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
