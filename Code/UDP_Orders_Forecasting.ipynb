{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Imports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Model_Tuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./General_Functions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code of the Demand Forecasting Pipeline, which is the process used to tune (train +\n",
    "validation) and back-test the models for each product (SKU) according to the different defined experiments. \n",
    "\n",
    "For each product this pipeline will train, tune, yield a best model (best set of hyperparameters) and backtest it for\n",
    "each of the different experiments; which means that every product will have as many \"best models\" as the number of\n",
    "experiments. The decision about which of these models to use as the final model is done by selecting the better on in\n",
    "terms of the validation WAPE.\n",
    "\n",
    "The result of this process consists of logging for all products the best model of each experiment into the Mlflow\n",
    "tracking API and generating the forecast for the back-testing period with each one of these, after that only the\n",
    "forecast corresponding to the best model among all the experiments is kept.\n",
    "\n",
    "The functions included are:\n",
    "\n",
    "| Function | Description |\n",
    "| -------- | ----------- |\n",
    "| `obtain_models` | for each product, obtains the best forecasting model per experiment |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initializing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment variables\n",
    "algorithms = [\"lightgbm\"]\n",
    "holidays = False\n",
    "num_evals = 2\n",
    "\n",
    "# Dates for validation\n",
    "start_val = \"2019-01-01\"\n",
    "end_val = \"2019-06-30\"\n",
    "\n",
    "# Dates for testing\n",
    "start_test = \"2019-07-01\"\n",
    "end_test = \"2019-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Defining search space of each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining search space for LightGBM\n",
    "params_lightgbm = {\n",
    "    \"n_estimators\":  hp.randint(\"n_estimators\", 15, 200),\n",
    "    \"max_depth\": hp.randint(\"max_depth\", 3, 50),\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", [0.01, 0.05, 0.1, 0.2]),\n",
    "    \"reg_alpha\": hp.choice(\"reg_alpha\", [0.2, 1, 5, 10]),\n",
    "    \"reg_lambda\": hp.choice(\"reg_lambda\", [0.2, 1, 5, 10])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Setting Mlflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining experiment path\n",
    "mlflow_exp = r\"/Users/n.garcia.aramouni@accenture.com/UDP_E2E_Forecasting/nerdearla_udp_shipments\"\n",
    "\n",
    "# Launching Mlflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Creating experiment or re-using it if already exists\n",
    "experiment = client.get_experiment_by_name(mlflow_exp)\n",
    "if experiment is None:\n",
    "    exp_id = mlflow.create_experiment(mlflow_exp)\n",
    "else:\n",
    "    exp_id = experiment.experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Defining modeling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_models(data, df_frds):\n",
    "    \"\"\"\n",
    "    For each product, obtains the best forecasting model per experiment; where an experiment is defined by an algorithm\n",
    "    in the context of the workshop.\n",
    "\n",
    "    Obtaining the best model of each experiment is done by performing hyperparameter tuning, which involves training and\n",
    "    validating multiple sets of hyperparameters to then select the best performing set according to a specific metric on\n",
    "    the validation set. Finally, all the best models from the experiments are used to generate the forecast for the\n",
    "    back-testing set and their performances on that set are recorded.\n",
    "\n",
    "    Results for each experiment such as train WAPE, validation WAPE and test WAPE are logged into Mlflow.\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "        data (pd.DataFrame): Dataset with the time series of the product.\n",
    "        df_frds (pd.DataFrame): Dataset with holidays.\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "        df_forecasts (pd.DataFrame): Table with the forecasts of the best models for the back-testing set.\n",
    "    \"\"\"\n",
    "    # Ensuring order of observations\n",
    "    data = data.sort_values(by=\"ds\", ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # Obtaining product info\n",
    "    sku = data[\"n_sku\"][0]\n",
    "    \n",
    "    # Order columns\n",
    "    regressors = [x for x in data.columns if x not in [\"ds\", \"n_sku\", \"y\"]]\n",
    "    data = data[[\"ds\"] + regressors + [\"y\"]]\n",
    "\n",
    "    # Splitting the series\n",
    "    df_trainval, df_test = split_series(data, start_test, end_test)\n",
    "\n",
    "    # Defining the output object\n",
    "    df_forecasts = pd.DataFrame()\n",
    "\n",
    "    # Looping over the algorithms\n",
    "    for algorithm in algorithms:\n",
    "        # Validating the algorithm to use\n",
    "        if algorithm == \"sarimax\":\n",
    "            search_space = params_sarimax\n",
    "        elif algorithm == \"prophet\":\n",
    "            search_space = params_prophet\n",
    "        elif algorithm == \"lightgbm\":\n",
    "            search_space = params_lightgbm\n",
    "\n",
    "        # Tuning the model\n",
    "        results = tune_ts_model(\n",
    "            algorithm, search_space, num_evals, df_trainval, start_val, end_val, holidays=holidays, df_frds=None\n",
    "        )\n",
    "\n",
    "        # Re-fitting model and generating forecast\n",
    "        df_fcst_alg, test_wape = refit_generate_forecast(\n",
    "            algorithm, results[\"params\"], df_trainval, df_test, holidays, df_frds\n",
    "        )\n",
    "\n",
    "        # Appending forecast to the output object\n",
    "        df_forecasts = df_forecasts.append(df_fcst_alg)\n",
    "\n",
    "        # Starting run and assigning tags\n",
    "        mlflow.start_run(experiment_id=exp_id, run_name=str(sku))\n",
    "        mlflow.set_tags(\n",
    "           {\n",
    "            \"experiment\": \"Nerdearla 2021\",\n",
    "            \"product\": sku,\n",
    "            \"algorithm\": algorithm\n",
    "           }\n",
    "        )\n",
    "\n",
    "        # Logging results in Mlflow\n",
    "        mlflow.log_metrics({\"train_wape\": results[\"train_wape\"], \"val_wape\": results[\"val_wape\"], \"test_wape\": test_wape})\n",
    "\n",
    "        # Ending run\n",
    "        mlflow.end_run()\n",
    "\n",
    "    # Adding identification column\n",
    "    df_forecasts[\"n_sku\"] = sku\n",
    "\n",
    "    return df_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training pipeline main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Loading the preprocessed data from Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = spark.read.csv(\"/FileStore/tables/raw_consumption_data_clean.csv\", header=\"true\")\n",
    "\n",
    "df = df\\\n",
    "    .withColumn(\"ds\", df['ds'].cast(DateType()))\\\n",
    "    .withColumn(\"y\", df['y'].cast(IntegerType()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, lead, col\n",
    "df = df.withColumn(\"lead\",lead(col(\"y\"),1).over(Window.partitionBy(\"n_sku\").orderBy(\"ds\")))\n",
    "df = df.withColumn(\"lag\",lag(col(\"y\"),1).over(Window.partitionBy(\"n_sku\").orderBy(\"ds\")))\n",
    "df = df.na.fill(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Performing modeling of SKUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining schema of the resulting dataframe:\n",
    "result_schema = StructType(\n",
    "    [\n",
    "     StructField(\"algorithm\", StringType(), False),\n",
    "     StructField(\"ds\", DateType(), False),\n",
    "     StructField(\"fcst\", FloatType(), False),\n",
    "     StructField(\"n_sku\", StringType(), False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Performing modeling of the DFUs\n",
    "df_fcsts = df.groupBy(\"n_sku\") \\\n",
    "    .applyInPandas(\n",
    "        lambda df: obtain_models(df, holidays),\n",
    "        result_schema\n",
    "    ) \\\n",
    "    .persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# Adding identification key of experiments\n",
    "df_fcsts = df_fcsts.withColumn(\"exp_key\", concat(df_fcsts[\"n_sku\"], lit(\"_\"), df_fcsts[\"algorithm\"]))\n",
    "display(df_fcsts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
